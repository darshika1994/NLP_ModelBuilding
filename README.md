# NLP_ModelBuilding
# Question - 1: How can we generate concise summaries of lengthy dialogue scenes in the Friends series?
# Answer - 1:
I have used the pre-trained BART model to effectively generate concise and coherent abstractive summaries of lengthy dialogue scenes from the Friends series, capturing the essential information while reducing the text length significantly.

BART (Bidirectional and Auto-Regressive Transformers) is a powerful model developed by Facebook AI for tasks like text summarization, translation, and text generation. It is called "Bidirectional and Auto-Regressive Transformers" because it incorporates two different methods for processing and generating text making the text contextually relevant, coherent and flexible.

  - Bidirectional: It means that the model reads the text from both directions—left-to-right and right-to-left (encoding stage).
  - Auto-regressive: It means that the model generates text one word at a time, using the previously generated words to predict the next word, starting with a special start token (decoding stage).

Implementation of BART on my text :

  - Filtering Long Sentences: Selecting subtitles that are longer than 250 characters (so that the model runs smoothly).
  - Combining Text: Merging these selected subtitles into a single string.
  - BART Model Flow:
    - Import libraries from transformers package -
        - BartTokenizer - encoding and decoding text
        - BartForConditionalGeneration - BART model used for generating summaries.
    - Load Pre-trained BART Model and Tokenizer
    - Tokenize and Encode the Text
    - Generate Summary
    - Decode the Summary

# Question - 2: How can the sentiment of characters be accessed while they are delivering their dialogues, visualize the sentiment distribution, and evaluate the model’s accuracy against a manually labeled dataset?
# Answer - 2:
I utilized the VADER model to assess the sentiment of characters as they delivered their dialogues. For visualization, I employed heatmaps and the WordCloud library to illustrate the sentiments. Additionally, I evaluated the accuracy of the VADER model against a manually labeled dataset, categorizing sentiments as Positive, Negative, and Neutral.

VADER (Valence Aware Dictionary and sentiment Reasoner):
  - Uses a pre-built dictionary where each word or phrase is assigned a sentiment score, for example, "happy" might have a positive score, while "sad" has a negative score.
  - VADER also considers modifiers like "very" or "not," which can change the intensity of the sentiment. For example, "very happy" would be more positive than just "happy," and "not happy" would be negative.
VADER then combines the scores of all words in a text to compute an overall sentiment score.
Implementation of VADER on my text :

Sentiment Analysis VADER Model: I used the VADER sentiment analysis model to compute sentiment scores of each character based on the words they used while they are delivering their dialogues. The sentiment scores helped classify each dialogue as Positive, Negative, or Neutral.

  - I applied the SentimentIntensityAnalyzer from the VADER model to each dialogue in the dataset to obtain sentiment scores.
  - I used the function comp_score to get the compound score and added it to the DataFrame.

Visualization:
  - Color-coded Sentences: I aimed to visually distinguish positive, negative, and neutral words in each dialogue for better readability. For this, I created Sentence_colored column, which contains HTML-formatted sentences with color-coded words based on their sentiment scores.
  - Word Clouds: All the words tagged as negative by the VADER model as colored as red, whereas positive words are colored as green using the WordCloud library.

Evaluated Model Accuracy: I compared the sentiment labels generated by the VADER model with manually labeled sentiments in the dataset.
  - Calculated Accuracy: I computed the accuracy of the VADER model by comparing its sentiment predictions with the manually labeled data.
  - Generated Classification Report and Confusion Matrix: I created a classification report and confusion matrix to assess the performance of the VADER model, using metrics such as precision, recall, and F1-score.

# Question - 3: How can the accuracy of sentiment classification in the Friends series using the VADER model be improved, and what strategies can be employed to address data imbalance?
# Answer - 3:
To enhance the low accuracy of the VADER model, I implemented an SVM model with TF-IDF feature extraction and applied SMOTE to handle data imbalance. I also evaluated the SVM model's accuracy against a manually labeled dataset, classifying sentiments into Positive, Negative, and Neutral categories.

SMOTE (Synthetic Minority Over-sampling Technique) works by creating new, similar data points for the less common class in a dataset to balance it out. Instead of just copying existing data points, SMOTE looks at a data point in the minority class and its closest neighbors, then creates new, slightly different data points between them. This helps models learn better when there’s an imbalance between classes, like when there are more negative sentiments than positive ones. Additionally, TF-IDF captures the importance of words and helps in distinguishing between different documents by assigning weights to terms


Implementation of SVM (TF-IDF) & SMOTE on my text :


Feature Extraction and Model Training:
  - TF-IDF Vectorization: I transformed the textual data into numerical features using TF-IDF (Term Frequency-Inverse Document Frequency) to capture the importance of words in the dialogues.
  - SVM Model: I trained an SVM (Support Vector Machine) model on the TF-IDF features to classify sentiments into Positive, Negative, or Neutral.
  - Handling Data Imbalance - SMOTE (Synthetic Minority Over-sampling Technique): I applied SMOTE to generate synthetic samples for underrepresented sentiment classes, ensuring a balanced dataset for training the SVM model.

Model Evaluation:

  - Performance Metrics: I evaluated the SVM model's performance using accuracy, classification reports, and confusion matrices to understand how well it classified sentiments compared to the manually labeled data.
  - Comparison: I compared the performance of the SVM model with the VADER model to determine if the improvements in accuracy were significant.

